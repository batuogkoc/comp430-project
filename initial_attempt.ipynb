{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ad075e-f91f-41f3-b38e-71d8f825dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "# general utilities\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "\n",
    "# deep learning frameworks\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# data processing, loading and splitting\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0871bf30-a6ca-4e88-ae98-0177aaf0363f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7905cb91-4651-4ea1-8352-ec17261d906f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted images: []\n"
     ]
    }
   ],
   "source": [
    "# CORRUPTED IMAGE DETECTION (to remove before training)\n",
    "image_folder = \"dataset/train\"\n",
    "corrupted_files = []\n",
    "\n",
    "for file in os.listdir(image_folder):\n",
    "    if file.endswith(('jpg', 'png')):\n",
    "        file_path = os.path.join(image_folder, file)\n",
    "        try:\n",
    "            img = Image.open(file_path)\n",
    "            img.verify() \n",
    "        except Exception:\n",
    "            corrupted_files.append(file_path)\n",
    "\n",
    "print(f\"Corrupted images: {corrupted_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f5e1e0-7c82-4a9e-944b-5fc00ada95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "class CaptchaDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith(('jpg', 'png'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n",
    "        label = os.path.splitext(self.image_files[idx])[0]  # Extract label from filename\n",
    "\n",
    "        # Apply the transform to the image (convert to tensor, normalize, etc.)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)  # Default conversion to tensor\n",
    "\n",
    "        # Convert label to indices (assuming 62 classes)\n",
    "        label_tensor = torch.tensor([self.char_to_index(c) for c in label])\n",
    "        return image, label_tensor\n",
    "\n",
    "    def char_to_index(self, char):\n",
    "        if char.isdigit():  # '0'-'9' -> 0-9\n",
    "            return int(char)\n",
    "        elif 'A' <= char <= 'Z':  # 'A'-'Z' -> 10-35\n",
    "            return ord(char) - ord('A') + 10\n",
    "        elif 'a' <= char <= 'z':  # 'a'-'z' -> 36-61\n",
    "            return ord(char) - ord('a') + 36\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid character: {char}\")\n",
    "\n",
    "\n",
    "# Image transformations (preprocessing)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 128)),  # Resize to uniform size\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Normalize to mean 0.5, std 0.5\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b7ff352-d8be-40e9-9192-aea6c932ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING and VALIDATION\n",
    "# Paths to dataset\n",
    "train_dataset_path = \"dataset/train\"\n",
    "val_dataset_path = \"dataset/val\"\n",
    "test_dataset_path = \"dataset/test\"\n",
    "# dataset initialization\n",
    "train_dataset = CaptchaDataset(train_dataset_path, transform=transform)\n",
    "val_dataset = CaptchaDataset(val_dataset_path, transform=transform)\n",
    "# data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7cd2926-2b39-4204-9206-755380dbc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN DEFINITON\n",
    "class CaptchaSolverCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CaptchaSolverCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "        nn.Linear(128 * 8 * 16, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(512, 5 * 62)  # 5-character CAPTCHA with 62 possible classes (0-9, A-Z, a-z)\n",
    ")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x.view(-1, 5, 62) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f163260a-dfcd-4f8f-92d5-cdc2cbad168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL AND LOSS FUNCTION INITIALIZATION\n",
    "model = CaptchaSolverCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d71ed67-8ac4-4acd-8b7c-e03ff4b60682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FUNCTION\n",
    "def train(model, train_loader, val_loader, epochs=20):\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"  # Ensure the correct device is set\n",
    "    model.to(device)  # Move the model to MPS\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)  \n",
    "            labels = labels.to(device) \n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(images)\n",
    "            loss = sum(criterion(outputs[:, i], labels[:, i]) for i in range(5))\n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)  # Move validation images to MPS\n",
    "                labels = labels.to(device)  # Move validation labels to MPS\n",
    "\n",
    "                outputs = model(images)\n",
    "                val_loss += sum(criterion(outputs[:, i], labels[:, i]).item() for i in range(5))\n",
    "                preds = outputs.argmax(dim=2)  # Get predictions\n",
    "                correct += (preds.cpu() == labels.cpu()).all(dim=1).sum().item()  # Compare predictions\n",
    "                total += labels.size(0)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Validation Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "              f\"Accuracy: {100 * correct/total:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbc450c2-0575-4bfd-a195-9ef4f563e13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 18.1815, Validation Loss: 14.5192, Accuracy: 0.15%\n",
      "Epoch [2/20], Loss: 14.4477, Validation Loss: 11.9827, Accuracy: 0.91%\n",
      "Epoch [3/20], Loss: 12.8517, Validation Loss: 10.5220, Accuracy: 2.10%\n",
      "Epoch [4/20], Loss: 11.8544, Validation Loss: 9.6562, Accuracy: 3.25%\n",
      "Epoch [5/20], Loss: 11.1618, Validation Loss: 9.0675, Accuracy: 4.22%\n",
      "Epoch [6/20], Loss: 10.6364, Validation Loss: 8.7015, Accuracy: 5.12%\n",
      "Epoch [7/20], Loss: 10.2191, Validation Loss: 8.4826, Accuracy: 5.97%\n",
      "Epoch [8/20], Loss: 9.8609, Validation Loss: 8.0876, Accuracy: 6.79%\n",
      "Epoch [9/20], Loss: 9.5729, Validation Loss: 7.9409, Accuracy: 7.60%\n",
      "Epoch [10/20], Loss: 9.2912, Validation Loss: 7.7116, Accuracy: 8.30%\n",
      "Epoch [11/20], Loss: 9.0658, Validation Loss: 7.8226, Accuracy: 8.28%\n",
      "Epoch [12/20], Loss: 8.8762, Validation Loss: 7.5882, Accuracy: 8.75%\n",
      "Epoch [13/20], Loss: 8.6776, Validation Loss: 7.3886, Accuracy: 10.01%\n",
      "Epoch [14/20], Loss: 8.5135, Validation Loss: 7.4213, Accuracy: 9.71%\n",
      "Epoch [15/20], Loss: 8.3786, Validation Loss: 7.3951, Accuracy: 9.85%\n",
      "Epoch [16/20], Loss: 8.2420, Validation Loss: 7.2235, Accuracy: 10.47%\n",
      "Epoch [17/20], Loss: 8.1219, Validation Loss: 7.2598, Accuracy: 10.86%\n",
      "Epoch [18/20], Loss: 7.9942, Validation Loss: 7.1900, Accuracy: 11.15%\n",
      "Epoch [19/20], Loss: 7.9103, Validation Loss: 7.1447, Accuracy: 11.33%\n",
      "Epoch [20/20], Loss: 7.8202, Validation Loss: 7.1338, Accuracy: 11.50%\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca95eb8-2792-48c4-9178-625b8c378c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
